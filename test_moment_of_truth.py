#!/usr/bin/env python3
"""
MOMENT OF TRUTH - Simple test script to verify the entire pipeline works
Tests: Extraction â†’ Conversion â†’ Database Storage
"""

import sys

# Import your existing classes (adjust imports as needed)
from extractors.extractor_matchday import MatchdayExtractor, CleanMatchExtractor
from database.orchestrators.match_orchestrator import MatchDataOrchestrator

def test_single_match_pipeline():
    """Test the complete pipeline with a single match"""
    
    print("ğŸ§ª MOMENT OF TRUTH - Testing Complete Pipeline")
    print("=" * 60)
    
    # Test URLs
    matchday_url = "https://www.transfermarkt.com/premier-league/spieltag/wettbewerb/GB1/saison_id/2023/spieltag/1"
    
    try:
        # Step 1: Extract matchday data
        print("1ï¸âƒ£ Extracting matchday data...")
        matchday_extractor = MatchdayExtractor()
        matchday_data = matchday_extractor.extract_from_transfermarkt_url(
            matchday_url, matchday=1, season="2023-24"
        )
        
        print(f"   âœ… Found {len(matchday_data.matches)} matches")
        
        if not matchday_data.matches:
            print("   âŒ No matches found - stopping test")
            return False
        
        # Pick the first match for detailed testing
        first_match = matchday_data.matches[0]
        print(f"   ğŸ“‹ Testing with: {first_match.home_team.get('name', 'Unknown')} vs {first_match.away_team.get('name', 'Unknown')}")
        print(f"   ğŸ”— Match report URL: {first_match.match_report_url}")
        
        # Step 2: Extract detailed match data
        print("\n2ï¸âƒ£ Extracting detailed match data...")
        match_extractor = CleanMatchExtractor()
        
        # Build full URL for detailed extraction
        detail_url = first_match.match_report_url
        if not detail_url.startswith('http'):
            detail_url = f"https://www.transfermarkt.com{detail_url}"
        
        print(f"   ğŸ¯ Scraping: {detail_url}")
        match_detail = match_extractor.extract_from_url(detail_url)
        
        print(f"   âœ… Match: {match_detail.home_team.name} vs {match_detail.away_team.name}")
        print(f"   âš½ Score: {match_detail.score.home_final}-{match_detail.score.away_final}")
        print(f"   ğŸ‘¥ Home lineup: {len(match_detail.home_lineup)} players")
        print(f"   ğŸ‘¥ Away lineup: {len(match_detail.away_lineup)} players")
        print(f"   ğŸ¥… Goals: {len(match_detail.goals)}")
        print(f"   ğŸŸ¨ Cards: {len(match_detail.cards)}")
        print(f"   ğŸ”„ Substitutions: {len(match_detail.substitutions)}")
        
        # Step 3: Check what URLs we have
        print("\n3ï¸âƒ£ Checking URL data...")
        print(f"   ğŸ“ Original match_report_url: {first_match.match_report_url}")
        
        extraction_metadata = match_detail.extraction_metadata or {}
        source_url = extraction_metadata.get('source_url')
        print(f"   ğŸ“ Extraction source_url: {source_url}")
        
        # Step 4: Test database save
        print("\n4ï¸âƒ£ Testing database save...")
        try:
            orchestrator = MatchDataOrchestrator(environment="production")
            
            if not orchestrator.is_available:
                print("   âŒ Database orchestrator not available")
                return False
            
            # Save matchday info
            print("   ğŸ’¾ Saving matchday data...")
            matchday_success = orchestrator.save_matchday_container(matchday_data)
            if matchday_success:
                print("   âœ… Matchday data saved successfully")
            else:
                print("   âš ï¸ Matchday data save failed")
            
            # Save detailed match
            print("   ğŸ’¾ Saving detailed match data...")
            match_success = orchestrator.save_match_detail(match_detail)
            if match_success:
                print("   âœ… Match detail saved successfully")
            else:
                print("   âŒ Match detail save failed")
                return False
            
        except Exception as e:
            print(f"   âŒ Database error: {e}")
            return False
        
        # Step 5: Verify database content
        print("\n5ï¸âƒ£ Verifying database content...")
        try:
            saved_match = orchestrator.get_match_by_id(match_detail.match_info.match_id)
            
            if saved_match:
                print("   âœ… Match found in database!")
                print(f"   ğŸ“‹ Match ID: {saved_match.get('match_id')}")
                print(f"   ğŸ  Home team: {saved_match.get('home_team_name')}")
                print(f"   ğŸƒ Away team: {saved_match.get('away_team_name')}")
                print(f"   âš½ Score: {saved_match.get('home_final_score')}-{saved_match.get('away_final_score')}")
                print(f"   ğŸ”— Match report URL: {saved_match.get('match_report_url')}")
                print(f"   ğŸ“ Source URL: {saved_match.get('source_url')}")
                print(f"   ğŸ“… Date: {saved_match.get('date')}")
                print(f"   ğŸŸï¸ Venue: {saved_match.get('venue')}")
                
                # Check if URLs are properly stored
                if saved_match.get('match_report_url') and saved_match.get('source_url'):
                    print("   âœ… Both URLs properly stored!")
                else:
                    print("   âš ï¸ URL fields missing or empty")
                    print(f"      match_report_url: {saved_match.get('match_report_url')}")
                    print(f"      source_url: {saved_match.get('source_url')}")
                
            else:
                print("   âŒ Match not found in database")
                return False
                
        except Exception as e:
            print(f"   âŒ Database verification error: {e}")
            return False
        
        print("\nğŸ‰ SUCCESS! Complete pipeline works!")
        return True
        
    except Exception as e:
        print(f"\nâŒ PIPELINE FAILED: {e}")
        import traceback
        print(traceback.format_exc())
        return False

def test_url_mapping():
    """Test just the URL mapping logic"""
    print("\nğŸ” Testing URL Mapping Logic")
    print("-" * 40)
    
    # Test data
    original_relative = "/spielbericht/index/spielbericht/4087933"
    converted_absolute = f"https://www.transfermarkt.com{original_relative}"
    
    print(f"Original relative URL: {original_relative}")
    print(f"Converted absolute URL: {converted_absolute}")
    
    # This is what should go in database:
    print(f"\nDatabase should store:")
    print(f"  match_report_url: {original_relative}")  # Keep original
    print(f"  source_url: {converted_absolute}")       # Store what we actually scraped

def main():
    """Run the moment of truth test"""
    
    print("ğŸš€ Starting Moment of Truth Test")
    print("This will test the complete extraction â†’ database pipeline")
    print()
    
    # Test URL mapping first
    test_url_mapping()
    
    # Test the complete pipeline
    success = test_single_match_pipeline()
    
    if success:
        print("\nğŸ¯ MOMENT OF TRUTH: PASSED! âœ…")
        print("The complete pipeline works as expected.")
    else:
        print("\nğŸ’¥ MOMENT OF TRUTH: FAILED! âŒ")
        print("Check the errors above and fix the issues.")
    
    return success

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ Test interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {e}")
        import traceback
        print(traceback.format_exc())
        sys.exit(1)